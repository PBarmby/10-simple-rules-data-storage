---
layout: article
title: Ten Simple Rules for Data Storage
tags: []
bibliography: resources/manuscript.bib
csl: resources/plos.csl
author:
 - family: Hart
   given: Edmund
   affiliation: 1
   email: emh@emhart.info
 - family: Barmby
   given: Pauline
   affiliation: 2
 - family: Hollister
   given: Jeffrey
   affiliation: 3
 - family: LeBauer
   given: David
   affiliation: 4
 - family: Michonneau
   given: Fran√ßois
   affiliation: 5

organization:
 - id: 1
   name: Univeristy of Vermont
   address: Department of Biology, Burlington
 - id: 2
   name: University of Western Ontario
   address: Department of Physics and Astronomy
 - id: 3
   name: US Environmental Protection Agency
   address: Atlantic Ecology Division
 - id: 4
   name: University of Illinois at Urbana-Champaign
   address: National Center for Supercomputing Applications and Institute for Genomic Biology
 - id: 5
   name: University of Florida
   address: iDigBio, Florida Museum of Natural History, Gainesville, FL 32611-7800

---

# Abstract {-}

\linenumbers

# Introduction {-}
Some example text with a citation [@goodman2014ten]

# Rule 1: Rule {-}

# Rule 2: Rule {Know your use case}

Researchers should know their use case and store data appropriately. Is this data collected and just being archived? Will it change regularly? How will those changes be logged (e.g. provenance if any)? Will this be shared via an API? Linked to a paper? What are the institutional restrictions? Can you use a commercial service like Dropbox or use a personally maintained system? Knowing the reason why you're sharing your data will constrain your choices here.

<!--
also see number 9, which will be aimed at big data
-->

# Rule 3: Rule {-}

# Rule 4: Rule {Store data in open formats}

To maximize accessibility and long-term value, data should be stored in file formats whose specificiations are freely-available. The exact file type will depend on the type of data being stored (e.g. numeric measurements, text, images, video) but the key idea is that data should not require proprietary software or hardware to access. Proprietary formats can change, maintaining organizations can go out of business, and changes in license fees could make access to data in  proprietary formats simply unaffordable. Examples of open data formats include
* CSV for tabular data
* HDF5 for (??)
* ?? for images
* (help me out here folks, it's been a long week)
and examples of closed formats include XLSX, DICOM, (again need more examples). At a minimum, data being stored for archival purposes should be stored in open formats, even if day-to-day processing uses closed formats.

# Rule 5: Rule {-}

# Rule 6: Rule {-}

# Rule 7: Rule {-}

# Rule 8: Rule {-}

# Rule 9: Rule {Data size matters /  requires special considerations}

* [#39](https://github.com/emhart/10-simple-rules-data-storage/issues/39) and related GH issues  [#16](https://github.com/emhart/10-simple-rules-data-storage/issues/16), [#19](https://github.com/emhart/10-simple-rules-data-storage/issues/16), [#25](https://github.com/emhart/10-simple-rules-data-storage/issues/25)

<!---
refs #16, #19, #25


needs to be different than rule 2
-->
* Size classes: <!-- perhaps a table?-->
	* larger than RAM
	* larger than HD space
	* larger than data storage server
* Storage method depends on the size of data; storage costs, transfer time, and computing costs can become substantial.
	* data generated by simulation and derived data should consider cost of storage vs. the cost of re-generating output.
	* For analyses of large data sets, the speed of reading and writing data can limt the speed of computation. <!-- many statistical analyses, summaries, relative to simulation models that generate lots of output from relatively few parameters-->
* Larger data sets that are actively used in analysis should be stored on a disk that is attached to a computer rather than being moved around between analysis and storage.
	* inactive data can be put in longer-term storage; this is less expensive, but can be slow to retrieve. Archiving of 'stale' files can be automated (and is at HPC centers).
* Data that is larger than memory can handle,
	* can be handled by 'big memory' nodes.
	* Computing can also be done 'in the database'
* Don't move (large data) around more than you have to - it can become inefficient, and make storage slower than necessary.
	* New tools make it easier to find and download data combined with reproducible scripts can lead to excessive and careless abuse of resources.
	* subset and compute on the server, in the database where possible. The dplyr R package does lazy eval; SQL can perform a wide range of data summaries, by groups, etc. On the other hand, it may be quicker to transfer normalized (e.g. 'flattening' a relational database can increase the size of data by orders of magnitude)
	* Use tools to store local 'cached' copies, instead of writing scripts that always download archived data. Only update data if there are changes.  * knitr has a cache argument that saves time in re-computing and in re-downloading.
* For data larger than a single hard drive disk, up to multiple servers
	* requires a meta-data server to allow fast access to distributed across many disks
* For very large data
	* it is not practical to store data
	* there are trade offs among cost, information content, and accessibility.

# Rule 10: Rule {Data should be stored in a machine readable format}

Not only data should be stored in an open format to ensure that data will be
easily and widely accessible (see Rule #4), they should also be stored in a
format that allows computers to make sense of it.

As datasets become increasingly larger, it is crucial that they can be parsed
efficiently. This is best achieved by using standard data formats that have
clear specifications (e.g., CSV, XML, JSON, HDF5). Such data formats can be
handled by a variety of programming languages as efficient and well-tested
libraries for parsing them are typically available. These standard data formats
also ensure interoperability, facilitate re-use, and reduce the chances of data
loss or mistakes being introduced during conversion between formats.

Because a computer will be able to import your data directly (i.e., without the
need for manual manipulation of your data), the script used to import and modify
the data can be made available and the origin of the data used in the analysis
will be evident. In turn, it will make the analysis more robust as there will be
no opportunity to introduce mistakes in the data, and it will make the analysis
reproducible.

To take full advantage of the data, it is important that it is structured such
that the data can be manipulated and analyzed easily, in other words that the
data is tidy (Wickham2014tidy). With tidy data, each variable is a column, each
observation is a row, and each type of observational unit is a table. When data
is organized like this, it reduces the duplication of information and it is
easier to subset or summarize the dataset to include the variables or
observations of interest.

<!-- include figure that shows example of untidy and equivalent tidy data? -->

To facilitate interoperability, it is best to use variable that can be mapped to
existing data standards. For instance, for biodiversity data, the
[Darwin Core Standard](http://www.tdwg.org/standards/450/) provides a set of
terms that describe observations, specimens, samples, and related information
for a taxa. Because each term is clearly defined and documented, each dataset
can use the terms consistently facilitating data sharing across applications and
disciplines.

With machine readable data, it is also easier to build an Application
Programming Interface (API) to query the dataset to retrieve a subset of
interest.

# Acknowledgements

National Center for Supercomputing Applications.
Software Carpentry Foundation.
iDigBio/NSF.

# Figure Legends {-}

Figures here:  Will need to figure out numbering...

# Tables {-}

Tables here:  Will need to figure out numbering...

\nolinenumbers

# References {-}
\bibdata{resources/manuscript.bib}
